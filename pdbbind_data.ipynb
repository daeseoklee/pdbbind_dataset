{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "\n",
    "import pybel\n",
    "from tfbio.data import Featurizer\n",
    "\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the extracted PDBbind dataset\n",
    "path = './pdbbind/v2016/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse and clean affinity data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s $path --out missing\n",
    "\n",
    "path=$1\n",
    "\n",
    "# Save binding affinities to csv file\n",
    "\n",
    "echo 'pdbid,-logKd/Ki' > affinity_data.csv\n",
    "cat $path/PDBbind_2016_plain_text_index/index/INDEX_general_PL_data.2016 | while read l1 l2 l3 l4 l5; do\n",
    "    if [[ ! $l1 =~ \"#\" ]]; then\n",
    "        echo $l1,$l4\n",
    "    fi\n",
    "done >> affinity_data.csv\n",
    "\n",
    "\n",
    "# Find affinities without structural data (i.e. with missing directories)\n",
    "\n",
    "cut -f 1 -d ',' affinity_data.csv | tail -n +2 | while read l;\n",
    "    do if [ ! -e $path/general-set-except-refined/$l ] &&  [ ! -e $path/refined-set/$l ]; then\n",
    "        echo $l;\n",
    "    fi\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = set(missing.split())\n",
    "len(missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affinity_data = pd.read_csv('affinity_data.csv', comment='#')\n",
    "affinity_data = affinity_data[~np.in1d(affinity_data['pdbid'], list(missing))]\n",
    "affinity_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaNs\n",
    "\n",
    "affinity_data['-logKd/Ki'].isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate core, refined, and general sets\n",
    "\n",
    "\n",
    "general_set = set(affinity_data['pdbid'])\n",
    "\n",
    "refined_set = ! grep -v '#' $path/PDBbind_2016_plain_text_index/index/INDEX_refined_data.2016 | cut -f 1 -d ' '\n",
    "refined_set = set(refined_set)\n",
    "\n",
    "core_set = ! grep -v '#' $path/PDBbind_2016_plain_text_index/index/INDEX_core.2016 | cut -f 1 -d ' '\n",
    "core_set = set(core_set) \n",
    "\n",
    "\n",
    "assert core_set & refined_set == core_set\n",
    "assert refined_set & general_set == refined_set\n",
    "\n",
    "len(general_set), len(refined_set), len(core_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude v 2013 core set - it will be used as another test set\n",
    "\n",
    "core2013 = ! grep -v '#' $path/PDBbind_2016_plain_text_index/index/INDEX_core_data.2013 | cut -f 1 -d ' '\n",
    "core2013 = set(core2013)\n",
    "\n",
    "affinity_data['include'] = True\n",
    "affinity_data.loc[np.in1d(affinity_data['pdbid'], list(core2013 & (general_set - core_set))), 'include'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affinity_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "affinity_data.loc[np.in1d(affinity_data['pdbid'], list(general_set)), 'set'] = 'general'\n",
    "\n",
    "affinity_data.loc[np.in1d(affinity_data['pdbid'], list(refined_set)), 'set'] = 'refined'\n",
    "\n",
    "affinity_data.loc[np.in1d(affinity_data['pdbid'], list(core_set)), 'set'] = 'core'\n",
    "\n",
    "affinity_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affinity_data[affinity_data['include']].groupby('set').apply(len).loc[['general', 'refined', 'core']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check affinity distributions\n",
    "\n",
    "grid = sns.FacetGrid(affinity_data[affinity_data['include']], row='set', row_order=['general', 'refined', 'core'],\n",
    "                     size=3, aspect=3)\n",
    "grid.map(sns.distplot, '-logKd/Ki');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affinity_data[['pdbid']].to_csv('pdb.ids', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "affinity_data[['pdbid', '-logKd/Ki', 'set']].to_csv('affinity_data_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = {'general': 'general-set-except-refined', 'refined': 'refined-set', 'core': 'refined-set'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s $path\n",
    "\n",
    "# Prepare pockets with UCSF Chimera - pybel sometimes fails to calculate the charges.\n",
    "# Even if Chimera fails to calculate several charges (mostly for non-standard residues),\n",
    "# it returns charges for other residues.\n",
    "\n",
    "path=$1\n",
    "\n",
    "for dataset in general-set-except-refined refined-set; do\n",
    "    echo $dataset\n",
    "    for pdbfile in $path/$dataset/*/*_pocket.pdb; do\n",
    "        mol2file=${pdbfile%pdb}mol2\n",
    "        if [[ ! -e $mol2file ]]; then\n",
    "            echo -e \"open $pdbfile \\n addh \\n addcharge \\n write format mol2 0 tmp.mol2 \\n stop\" | chimera --nogui\n",
    "            # Do not use TIP3P atom types, pybel cannot read them\n",
    "            sed 's/H\\.t3p/H    /' tmp.mol2 | sed 's/O\\.t3p/O\\.3  /' > $mol2file\n",
    "        fi\n",
    "    done \n",
    "done > chimera_rw.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = Featurizer()\n",
    "charge_idx = featurizer.FEATURE_NAMES.index('partialcharge')\n",
    "\n",
    "with h5py.File('%s/core2013.hdf' % path, 'w') as g:\n",
    "    j = 0\n",
    "\n",
    "    for dataset_name, data in affinity_data.groupby('set'):\n",
    "\n",
    "        print(dataset_name, 'set')\n",
    "        i = 0\n",
    "        ds_path = dataset_path[dataset_name]\n",
    "\n",
    "\n",
    "        with h5py.File('%s/%s.hdf' % (path, dataset_name), 'w') as f:\n",
    "            for _, row in data.iterrows():\n",
    "\n",
    "                name = row['pdbid']\n",
    "                affinity = row['-logKd/Ki']\n",
    "\n",
    "                ligand = next(pybel.readfile('mol2', '%s/%s/%s/%s_ligand.mol2' % (path, ds_path, name, name)))\n",
    "                # do not add the hydrogens! they are in the strucutre and it would reset the charges\n",
    "\n",
    "                try:\n",
    "                    pocket = next(pybel.readfile('mol2', '%s/%s/%s/%s_pocket.mol2' % (path, ds_path, name, name)))\n",
    "                    # do not add the hydrogens! they were already added in chimera and it would reset the charges\n",
    "                except:\n",
    "                    warnings.warn('no pocket for %s (%s set)' % (name, dataset_name))\n",
    "                    continue\n",
    "\n",
    "                ligand_coords, ligand_features = featurizer.get_features(ligand, molcode=1)\n",
    "                assert (ligand_features[:, charge_idx] != 0).any()\n",
    "                pocket_coords, pocket_features = featurizer.get_features(pocket, molcode=-1)\n",
    "                assert (pocket_features[:, charge_idx] != 0).any() \n",
    "\n",
    "                centroid = ligand_coords.mean(axis=0)\n",
    "                ligand_coords -= centroid\n",
    "                pocket_coords -= centroid\n",
    "\n",
    "                data = np.concatenate((np.concatenate((ligand_coords, pocket_coords)),\n",
    "                                       np.concatenate((ligand_features, pocket_features))), axis=1)\n",
    "\n",
    "                if row['include']:\n",
    "                    dataset = f.create_dataset(name, data=data, shape=data.shape, dtype='float32', compression='lzf')\n",
    "                    dataset.attrs['affinity'] = affinity\n",
    "                    i += 1\n",
    "                else:\n",
    "                    dataset = g.create_dataset(name, data=data, shape=data.shape, dtype='float32', compression='lzf')\n",
    "                    dataset.attrs['affinity'] = affinity\n",
    "                    j += 1\n",
    "\n",
    "        print('prepared', i, 'complexes')\n",
    "    print('excluded', j, 'complexes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('%s/core.hdf' % path, 'r') as f, \\\n",
    "     h5py.File('%s/core2013.hdf' % path, 'r+') as g:\n",
    "    for name in f:\n",
    "        if name in core2013:\n",
    "            dataset = g.create_dataset(name, data=f[name])\n",
    "            dataset.attrs['affinity'] = f[name].attrs['affinity']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protein data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_data = pd.read_csv('%s/PDBbind_2016_plain_text_index/index/INDEX_general_PL_name.2016' % path,\n",
    "                           comment='#', sep='  ', engine='python', na_values='------',\n",
    "                           header=None, names=['pdbid', 'year', 'uniprotid', 'name'])\n",
    "\n",
    "protein_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we assume that PDB IDs are unique\n",
    "assert ~protein_data['pdbid'].duplicated().any()\n",
    "\n",
    "protein_data = protein_data[np.in1d(protein_data['pdbid'], affinity_data['pdbid'])]\n",
    "\n",
    "# check for missing values\n",
    "protein_data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_data[protein_data['name'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix rows with wrong separators between protein ID and name\n",
    "\n",
    "for idx, row in protein_data[protein_data['name'].isnull()].iterrows():\n",
    "    uniprotid = row['uniprotid'][:6]\n",
    "    name = row['uniprotid'][7:]\n",
    "    protein_data.loc[idx, ['uniprotid', 'name']] = [uniprotid, name]\n",
    "\n",
    "protein_data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_data.to_csv('protein_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "pafnucy_env",
   "language": "python",
   "name": "pafnucy_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
